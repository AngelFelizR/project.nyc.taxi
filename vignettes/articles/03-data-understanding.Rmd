---
title: "Data Understanding"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Understanding}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
params:
  UpdateSample: FALSE
  UpdateGeoPoints: FALSE
---


```{r, include = FALSE}
TrainingPath <- here::here("raw-data/TrainingTrips.fst")
ExistTrainingData <- file.exists(TrainingPath)

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library('project.nyc.taxi')
```

After completing the [business understanding](https://angelfelizr.github.io/project.nyc.taxi/articles/02-business-understanding.html) phase we are ready to perform the **data understanding** phase by performing an EDA with the following steps:

1.  Exploring the individual distribution of variables.
2.  Taking a subset of the data to fit in RAM.
3.  Exploring correlations between predictors and target variable.
4.  Exploring correlations between predictors.

In order to:

-   Ensure data quality
-   Identify key predictors
-   Detect multicollinearity
-   Guide model choice and feature engineering

## Setting the environment up

To setting the `R` environment up we just need to apply the following 5 steps:

1.  Loading the packages to use.

```{r warning = FALSE, message = FALSE}
library(here)
library(data.table)
library(ggplot2)
library(scales)
library(forcats)
library(lubridate)
library(dplyr)
library(arrow)
library(recipes)

options(datatable.print.nrows = 15,
        digits = 4)
```

2.  Creating an Arrow connection object to perform some **manipulations in disk** before taking the data into the RAM memory.

```{r}
data_path <- here("raw-data/trip-data/year=2022")

NycTrips2022 <- open_dataset(data_path)

dim(NycTrips2022) |> comma()

NycTrips2022
```

3.  Importing the zone code description with `lat` and `long`.

```{r, eval=params$UpdateGeoPoints}
ZoneCodesArcgis <- 
  fread(here("raw-data/taxi_zone_lookup.csv"),
        colClasses = c("integer",
                       "character",
                       "character",
                       "character")
  )[, Address := paste(Zone,
                       Borough,
                       "New York",
                       "United States",
                       sep = ", ")
  ][, tidygeocoder::geocode(.SD,
                            address = Address,
                            method = 'arcgis')]

setDT(ZoneCodesArcgis)

glimpse(ZoneCodesArcgis)
```

```{r, echo=FALSE}

if(params$UpdateGeoPoints){
  
  usethis::use_data(ZoneCodesArcgis,
                    overwrite = TRUE,
                    compress = "xz",
                    version = 3)
  
}else{
  
  glimpse(ZoneCodesArcgis)

}

```

4. Validating the added latitude and longitude, by plotting all the points in an interactive map from we can highlight the following issues:

  - The location suggested for the Newark Airport is not correct.
  - More than one zone is represented by one location.

```{r}
BoroughColors <- c(
  'Manhattan' = '#e41a1c',
  'Queens' = '#377eb8',
  'Brooklyn'= '#4daf4a',
  'Bronx' = '#984ea3',
  'Staten Island' = '#ff7f00',
  'EWR' = '#a65628'
)

plot_map(
  ZoneCodesArcgis,
  lng_var = "long",
  lat_var = "lat",
  color_var = "Borough",
  color_palette = BoroughColors,
  radius = 3,
  label_var = "Zone"
)
```

Now we can see the new map after solving the prior problems.

```{r}
ZoneCodesArcgisClean <- clean_zone_manually(ZoneCodesArcgis)

plot_map(
  ZoneCodesArcgisClean,
  lng_var = "long",
  lat_var = "lat",
  color_var = "Borough",
  color_palette = BoroughColors,
  radius = 3,
  label_var = "Zone"
)
```

5.  Counting the number of trips for each code, collecting and translating the zone codes.

```{r}
TripsZoneDistribution <-
  NycTrips2022 |>
  count_pct(PULocationID, DOLocationID) |>
  add_zone_description(zone_dt = ZoneCodesArcgisClean,
                       start_id_col = "PULocationID",
                       end_id_col = "DOLocationID",
                       zone_id_col = "LocationID")

glimpse(TripsZoneDistribution)
```

## Individual distributions

### Categorical variables

Let's starting counting and checking the proportions related to each category.

#### company

The majority number of trips are done by *Uber* (HV003) and the rest for *Lyft*.

```{r}
NycTrips2022 |> 
  count_pct(company = case_when(hvfhs_license_num == "HV0002" ~ "Juno",
                                hvfhs_license_num == "HV0003" ~ "Uber",
                                hvfhs_license_num == "HV0004" ~ "Via",
                                hvfhs_license_num == "HV0005" ~ "Lyft"))
```

> To improve interpretavility we need to make the same translation after sampling the data.

#### dispatching_base_num

For most of the trips the dispatching number are **B03404** and **B03406**. The remaining codes represent a small proportion of observations.

```{r}
NycTrips2022 |>
  count_pct(dispatching_base_num)
```

#### originating_base_num

For most of the trips the originating number is **B03404** and the second is missing. The remaining codes represent a small proportion of observations.

```{r}
NycTrips2022 |>
  count_pct(originating_base_num)
```

#### access_a_ride_flag

Must of the trips doesn't provide any information related to whether they were administered on behalf of the Metropolitan Transportation Authority (MTA) and the remaining confirm that there aren't.

```{r}
NycTrips2022 |>
  count_pct(access_a_ride_flag)
```

#### shared_request_flag

Most of passengers **don't request** to a shared/pooled ride.

```{r}
NycTrips2022 |>
  count_pct(shared_request_flag)
```

#### shared_match_flag

Most of passengers don't request to a shared/pooled ride, but even **fewer achieved to share the ride**.

```{r}
NycTrips2022 |> 
  count_pct(shared_match_flag)
```

#### wav_request_flag

It's really **unusual to request a wheelchair-accessible** vehicle.

```{r}
NycTrips2022 |> 
  count_pct(wav_request_flag)
```

#### wav_match_flag

It's really unusual to request a wheelchair-accessible vehicle, but we have more tips take place in wheelchair-accessible vehicles, so **it seems that the service has the capacity to meet the demand**.

```{r}
NycTrips2022 |> 
  count_pct(wav_match_flag)
```

#### start_service_zone

**96% of trips** start in *Boro Zone* and *Yellow Zone*.

```{r}
TripsZoneDistribution |>
  count_pct(start_service_zone, wt =  n)
```

#### end_service_zone

**91% of trips** start in *Boro Zone* and *Yellow Zone*.

```{r}
TripsZoneDistribution |>
  count_pct(end_service_zone, wt =  n)
```

#### start_borough

**87% of trips** start in *Manhattan*, *Brooklyn* or *Queens*.

```{r}
TripsZoneDistribution |>
  count_pct(start_borough, wt =  n)
```

#### start_zone {.tabset}

To gain a deeper understanding of the starting trips at the zone level, we'll create a **wrapper function** to display the top 5 most significant zones and generate an interactive map.

The map will illustrate the location of each zone in relation to the remaining locations within each borough.

##### Manhattan

Here we can see how the _East Village_ zone is surrounded by _(Meatpacking) West Village_ and _Clinton (East|West)_, so most of the activity happens in those places. The remaining top zones ( _Upper East Side (North|South)_ and _Central Harlem (North)_ ) are surrounding the **Central Park**.

All these places are very popular for local and international tourist.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Manhattan",
                    borough_color = BoroughColors[["Manhattan"]],
                    top_color = "purple",
                    col_prefix  = "start_")
```

##### Brooklyn

In this Borough, the most important starting zones are concentrated in the north part. _Bushwick (North|South)_, _Crown Heights (North|South)_, _Williamsburg (North Side)_ and _Park Slope_ form an **"U"** shape, the farthest zone is the _East New York_.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Brooklyn",
                    borough_color = BoroughColors[["Brooklyn"]],
                    top_color = "purple",
                    col_prefix  = "start_")
```

##### Queens

Most of the most important starting zones are concentrated in north part of this Borough and the only exception is the _JFK Airport_ in the south.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Queens",
                    borough_color = BoroughColors[["Queens"]],
                    top_color = "purple",
                    col_prefix  = "start_")
```

#### end_borough

**82% of trips** end in *Manhattan*, *Brooklyn* or *Queens*.

```{r}
TripsZoneDistribution |>
  count_pct(end_borough, wt =  n)
```

#### end_zone {.tabset}

##### Manhattan

If we focus our attention to the places where the trips end we see the new zone _East Harlem (North|South)_ which get the top 5 moving the _East Village_ to the sixth position.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Manhattan",
                    borough_color = BoroughColors[["Manhattan"]],
                    top_length = 6,
                    top_color = "purple",
                    col_prefix  = "end_")
```

##### Brooklyn

We also can see the same pattern from the ending zone perspective.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Brooklyn",
                    borough_color = BoroughColors[["Brooklyn"]],
                    top_color = "purple",
                    col_prefix  = "end_")
```

##### Queens

For the ending perspective, now the _JFK Airport_ present more trips than the _LaGuardia Airport_.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Queens",
                    borough_color = BoroughColors[["Queens"]],
                    top_color = "purple",
                    col_prefix  = "end_")
```

### Time variables

As we want to predict the **profit rate** related to each trip, we just need to consider for this analysis `request_datetime` as is the only variable available to the driver before starting a trip and the `dropoff_datetime` the point used to defining the end of a trip.

We know that a taxi trip takes more less than a day doesn't make more sense to explore each variable related to time. Instead, it is better to consecrate the efforts to understand the distribution of the exploratory variable later to explore the difference between `request_datetime` and `dropoff_datetime` as a **numeric variable**.

To describe this variable, we decomposed it in different parts and count the number trips by each element and store the summary as a `data.table` to explore each part using visualizations.

```{r}
RequestTimeSummary <-
  NycTrips2022 |>
  mutate(request_date = as_date(request_datetime)) |>
  count(request_month = floor_date(request_date, unit = "month"),
        request_week = floor_date(request_date, unit = "week"),
        request_day = day(request_date),
        request_weekday = wday(request_date, week_start = 1),
        request_hour = hour(request_datetime)) |>
  collect() |>
  as.data.table()
```

#### request_datetime by month

In the next chart, we can see that the number trips keeps almost constant must of the year, but we have some fewer trips during the first 2 months and some extra trips in October and December.

```{r}
RequestTimeSummary[year(request_month) == 2022, 
                   .(n = sum(n)),
                   by = "request_month"] |>
  ggplot(aes(request_month, n))+
  geom_line(color = "grey60",
            linewidth = 0.9)+
  geom_point(color = "dodgerblue4",
             size = 3)+
  scale_x_date(date_labels = "%y-%m",
               date_breaks = "2 months")+
  scale_y_continuous(labels = comma_format(),
                     breaks = breaks_width(2e6))+
  labs(title = "Distribution of Trips by Month",
       x = "Trips Request Month",
       y = "Number of Trips")+
  expand_limits(y = 0)+
  theme_light()+
  theme(plot.title = element_text(face = "bold", hjust = 0.5),
        panel.grid.minor = element_blank(),
        legend.position = "top")
```

#### request_datetime by week

By breaking the months into weeks we can confirm we have fewer trips in the first 2 months, in relation to October we don't see a big change in he number of trips we see is that it has more weeks than November, but December keeps having more trips than normal in the first 2 weeks.

```{r}
RequestTimeSummary[year(request_month) == 2022, 
                   .(n = sum(n)),
                   by = "request_week"] |>
  ggplot(aes(request_week, n))+
  geom_line(color = "grey60",
            linewidth = 0.9)+
  geom_point(color = "dodgerblue4",
             size = 3)+
  scale_x_date(date_labels = "%y-%m",
               date_breaks = "month")+
  scale_y_continuous(labels = comma_format(),
                     breaks = breaks_width(5e5))+
  labs(title = "Distribution of Trips by Week",
       x = "Trips Request Week",
       y = "Number of Trips")+
  expand_limits(y = 0)+
  theme_light()+
  theme(plot.title = element_text(face = "bold", hjust = 0.5),
        panel.grid.minor = element_blank(),
        legend.position = "top")
```

#### request_datetime by month day

If we explore the number of trips by month day we can not see any consistent pattern after plotting a line with total of trips for each month.

```{r}
RequestTimeSummary[year(request_month) == 2022, 
                   .(n = sum(n)),
                   by = .(request_month = format(request_month, "%B"),
                          request_day)] |>
  ggplot(aes(request_day, n))+
  geom_line(aes(group = request_month),
            color = "gray60",
            linewidth = 0.1)+
  geom_smooth(method = 'loess',
              formula = 'y ~ x',
              se = FALSE,
              linewidth = 1.2)+
  scale_x_continuous(breaks = breaks_width(5))+
  scale_y_continuous(labels = comma_format())+
  expand_limits(y = 0)+
  labs(title = "Number of Trips by Month Day",
       y = "Number of Trips",
       x = "Day of Month")+
  theme_light()+
  theme(panel.grid = element_blank(),
        plot.title = element_text(face = "bold"))
```

#### request_datetime by week day

By if we change the month day in the prior chart with week day we can find that the number of trips trends to be higher Fridays and Saturdays.

```{r,warning=FALSE}
RequestTimeSummary[year(request_month) == 2022, 
                   .(n = sum(n)),
                   by = .(request_month = format(request_month, "%B"),
                          request_weekday)] |>
  ggplot(aes(request_weekday, n))+
  geom_line(aes(group = request_month),
            color = "gray60")+
  geom_smooth(method = 'loess',
              formula = 'y ~ x',
              se = FALSE,
              linewidth = 1.2)+
  scale_x_continuous(breaks = breaks_width(1),
                     labels = factor_weekday)+
  scale_y_continuous(labels = comma_format())+
  expand_limits(y = 0)+
  labs(title = "Number of Trips by Week Day",
       y = "Number of Trips",
       x = "Day of Week")+
  theme_light()+
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))
```

#### request_datetime by week day and hour

To understand better what is happening Fridays and Saturdays let's break each week day by hour. In the next chart, we can see how the higher number of trips start at 17:00 and end at 1:00 of next day for Fridays and Saturdays.

```{r}
RequestTimeSummary[year(request_month) == 2022, 
                   .(n = sum(n)),
                   by = .(request_hour = 
                            factor(request_hour) |> fct_rev(), 
                          request_weekday = factor_weekday(request_weekday))
  ][, n_million := n/1e6 ] |>
  ggplot(aes(request_weekday, request_hour))+
  geom_tile(aes(fill = n),
            color = "black",
            linewidth = 0.005)+
  geom_text(aes(label = comma(n_million, accuracy = 0.1, suffix = " M")))+
  scale_fill_gradient(low = "white", 
                      high = "dodgerblue4",
                      labels= comma_format())+
  scale_x_discrete(position = "top") +
  labs(title = "Number of Trips by Hour and Week Day",
       fill = "Number of Trips",
       x = "Request Week Day",
       y = "Request Hour") +
  theme_classic() +
  theme(plot.title = element_text(face = "bold"),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        axis.text = element_text(color = "black"))
```

### Numeric variables

Last but not least, it time check explore the individual distribution of numeric variable based of summary metrics by using the custom `compute_num_summary` to use all the power under the `arrow` package.

#### trip_miles

Below we can see that the min distance was **0 miles** which can be possible if the trip duration was also short, but we need to check those cases and the higher distance was **634 miles**, which can be a valid outlier taking in consideration that the higher whisker is 13.2 miles.

```{r}
NycTrips2022 |>
  compute_num_summary(trip_miles)
```

We can also check that only 24.4K trips are over 100 miles.

```{r}
NycTrips2022 |>
  count_pct(trip_miles_status = case_when(trip_miles < 100 ~ "Normal trips",
                                          trip_miles >= 100 ~ "Long Trips"),
            digits = 4L)
```

But even just taking the long trips we can see that 98% of those trips were shorter than **300 miles**, those trips doesn't look like good examples to train the model.

```{r}
NycTrips2022 |>
  filter(trip_miles >= 100) |>
  count_pct(miles_interval = case_when(trip_miles >= 600 ~ ">=600",
                                       trip_miles >= 500 ~ "[500, 600)",
                                       trip_miles >= 400 ~ "[400, 500)",
                                       trip_miles >= 300 ~ "[300, 400)",
                                       trip_miles >= 200 ~ "[200, 300)",
                                       trip_miles >= 150 ~ "[150, 200)",
                                       trip_miles >= 130 ~ "[130, 150)",
                                       trip_miles >= 115 ~ "[115, 130)",
                                       trip_miles >= 100 ~ "[100,115)"),
            digits = 4L)
```

#### base_passenger_fare

Just by checking the summary statistics we can see to important problems:

-   A base fare can not be negative
-   A base of almost \$5k is too much

```{r}
NycTrips2022 |>
  compute_num_summary(base_passenger_fare)
```

By checking the closer to the distribution we can confirm that 99% of trips present a base fare from 5 to 50 dollars, but any fare higher that 750 dollars seems weird won't be used to train the model.

```{r}
NycTrips2022 |>
  count_pct(base_passenger_fare_interval = 
              case_when(base_passenger_fare >= 4000 ~ ">=4000",
                        base_passenger_fare >= 3000 ~ "[3000, 4000)",
                        base_passenger_fare >= 2000 ~ "[2000, 3000)",
                        base_passenger_fare >= 1000 ~ "[1000, 2000)",
                        base_passenger_fare >= 750 ~ "[750, 1000)",
                        base_passenger_fare >= 500 ~ "[500, 750)",
                        base_passenger_fare >= 400 ~ "[400, 500)",
                        base_passenger_fare >= 300 ~ "[300, 400)",
                        base_passenger_fare >= 200 ~ "[200, 300)",
                        base_passenger_fare >= 100 ~ "[100, 200)",
                        base_passenger_fare >= 50 ~ "[50, 10)",
                        base_passenger_fare >= 5 ~ "[5, 50)",
                        base_passenger_fare >= 0 ~ "[0, 5)",
                        TRUE ~ "<0"),
            digits = 8L)
```

#### driver_pay

Just by checking the summary statistics we can confirm that also `driver_pay` is sharing the same problems that `base_passenger_fare`.

```{r}
NycTrips2022 |>
  compute_num_summary(driver_pay)
```

And after checking the distribution we can confirm that paying more than 750 must be a mistake that we don't want to use for training the model.

```{r}
NycTrips2022 |>
  count_pct(driver_pay_interval = 
              case_when(driver_pay >= 4000 ~ ">=4000",
                        driver_pay >= 3000 ~ "[3000, 4000)",
                        driver_pay >= 2000 ~ "[2000, 3000)",
                        driver_pay >= 1000 ~ "[1000, 2000)",
                        driver_pay >= 750 ~ "[750, 1000)",
                        driver_pay >= 500 ~ "[500, 750)",
                        driver_pay >= 400 ~ "[400, 500)",
                        driver_pay >= 300 ~ "[300, 400)",
                        driver_pay >= 200 ~ "[200, 300)",
                        driver_pay >= 100 ~ "[100, 200)",
                        driver_pay >= 50 ~ "[50, 10)",
                        driver_pay >= 5 ~ "[5, 50)",
                        driver_pay >= 0 ~ "[0, 5)",
                        TRUE ~ "<0"),
            digits = 8L)
```

#### tips

The column looks really good, it makes sense that for more than 75% of trips the tip is 0 as it is not mandatory, but having a trip with a \$294 tip is not hard to believe.

```{r}
NycTrips2022 |>
  compute_num_summary(tips)
```

After braking the `tips` we can see that `80%` of trips don't present any tip and must of the tips are lower than **50 dollars**, so gets really hard to achieved more than that based on tips.

```{r}
NycTrips2022 |>
  count_pct(tips_interval = case_when(tips >= 250 ~ ">=250",
                                      tips >= 200 ~ "[200, 250)",
                                      tips >= 150 ~ "[150, 200)",
                                      tips >= 100 ~ "[100, 150)",
                                      tips >= 50 ~ "[50, 100)",
                                      tips > 0 ~ "(0, 50)",
                                      TRUE ~ "0"),
            digits = 8L)
```

#### trip_minutes

After taking the difference in minutes between the request time and the drop off time in minutes, we can see negative trips and some other much longer than an hour.

```{r}
NycTrips2022 |>
  mutate(trip_minutes = arrow_minutes_between(request_datetime, dropoff_datetime)) |>
  compute_num_summary(trip_minutes)
```

Now we can see that 99% of trips takes less than 2 hours and only 152,049 trips have more than 2 hour trip.

```{r}
NycTrips2022 |>
  mutate(trip_minutes = arrow_minutes_between(request_datetime, dropoff_datetime)) |>
  count_pct(trip_minutes_status = case_when(trip_minutes < 0 ~ "Negative",
                                            trip_minutes <= 60   ~ "1 hour or less",
                                            trip_minutes <= (60*2) ~ "(1:00, 2:00]",
                                            trip_minutes <= (60*3) ~ "(2:00, 3:00]",
                                            trip_minutes > (60*3) ~ ">3 hours"),
            digits = 6L)
```




## Sampling data

Due our limited compute power capacity we will need to sample the data to be able to unlock all R capacities.

As most of the trips take place between Manhattan, Brooklyn and Queens, we are going to focus first in those zones.

```{r include=FALSE, eval=!params$UpdateSample}
TrainingTrips <- fst::read_fst(TrainingPath, as.data.table = TRUE)
```

```{r, eval=params$UpdateSample}
BoroughToKeep <- c("Manhattan", "Brooklyn", "Queens")

TrainingTrips <-
  dir(data_path,
      full.names = TRUE,
      recursive = TRUE) |>
  lapply(sample_parquet,
         valid_zones = ZoneCodesArcgisClean[Borough %chin% BoroughToKeep, LocationID],
         prob = 0.01) |>
  rbindlist()
```

```{r, include=FALSE, eval=params$UpdateSample}
fst::write_fst(TrainingTrips,
               "raw-data/TrainingTrips.fst",
               100)
```

## Solving quality problems

```{r}
TrainingTripsCleaned <-
  TrainingTrips |>
  add_zone_description(zone_dt = ZoneCodesArcgisClean,
                       start_id_col = "PULocationID",
                       end_id_col = "DOLocationID",
                       zone_id_col = "LocationID") |>
  apply_base_cleaning()

glimpse(TrainingTripsCleaned)
```

```{r include=FALSE}
rm(TrainingTrips)
```

```{r}
TrainingTripsCleaned[, .N,
                     by = c("trip_minutes",
                            "profit_rate")] |>
  ggplot(aes(trip_minutes, profit_rate))+
  geom_point(aes(alpha = N))+
  scale_y_continuous(labels = dollar_format(),
                     trans = "log2")+
  theme_light()
```


```{r}
TrainingTripsCleaned[, .SD, .SDcols = patterns("profit_rate|company|_num|_flag")] |>
  melt(id.vars = "profit_rate",
       value.factor = TRUE) |>
  ggplot(aes(value, profit_rate))+
  geom_boxplot()+
  facet_wrap(~variable)
```

## Correlations between predictors and target variable

Before exploring correlations we need to transforming the sampled data by:

-   Solving quality problems.
-   Transforming categorical variables into boolean ones.
-   Transforming zone code to factors.
-   Adding new features.

## start_service vs end_service zones

```{r}
#| echo: false
#| include: false

ZoneUniqueComb <-
  TripsZoneDistribution[ 
    !start_borough %chin% c("Staten Island", "Unknown", "EWR") & 
      !end_borough %chin% c("Staten Island", "Unknown", "EWR"),
    unique(.SD),
    .SDcols = c("start_zone", "end_zone")
  ][, .N] |>
  comma()

EndingUniqueZones <-
  TripsZoneDistribution[ 
    !start_borough %chin% c("Staten Island", "Unknown", "EWR") & 
      !end_borough %chin% c("Staten Island", "Unknown", "EWR"),
    uniqueN(end_zone),] |>
  comma()
```

-   `start_zone` and `end_zone`: As our data has `r ZoneUniqueComb` rows of relations between both columns, we opted to transform data in a way what each unique zone represent a row reducing the points to be plotted to only `r EndingUniqueZones` by following the next steps:

    1.  Summarizing the total number of trips for each **starting point** independently to its destination
    2.  Summarizing the total number of trips for each **ending point** independently to its origin.
    3.  Joining both tables into one.

```{r}
# 1. Summarizing Staring Zones
StartingZonesCount <-
  TripsZoneDistribution[
    !start_borough %chin% c("Staten Island", "Unknown", "EWR") &
      !end_borough %chin% c("Staten Island", "Unknown", "EWR"),
    .(start_trips = sum(n)),
    by =  .(borough = start_borough, 
            zone = start_zone)
  ]

# 2. Summarizing Ending Zones
EndingZonesCount <-
  TripsZoneDistribution[
    !start_borough %chin% c("Staten Island", "Unknown", "EWR") &
      !end_borough %chin% c("Staten Island", "Unknown", "EWR"), 
    .(end_trips = sum(n)),
    by =  .(borough = end_borough, 
            zone = end_zone)
  ]

# 3. Inner Joining Starting and Ending Zones Counts
JoinedZonesCount <-
  StartingZonesCount[
    EndingZonesCount,
    on = c("borough", "zone"),
    nomatch = 0
  ]
```

Once we have a much simpler data to work with, we is easy to confirm with the next linear regression that `start_trips` and `end_trips` has almost the same values the model has an slope of one. That means that in must of cases **if someone takes a taxi to go to any place it's really likely to take another taxi back**.

```{r}
lm(end_trips ~ start_trips, 
   data = JoinedZonesCount) |>
  summary()
```

Let's now explore the zones where there is **no balance** between the `start_trips` and the `end_trips` in the most visited zoned of each Borough. To do so, we defined the rate `end_trips`/`start_trips` and highlight zones with lower rate than the 15% percentile or higher rate than 85% percentile.

```{r}
# Creating dataset to plot
ZonesCountToPlot <-
  copy(JoinedZonesCount)[
    j = `:=`(ending_starting_rate = end_trips/start_trips,
             borough = fct_reorder(borough, -end_trips, .fun = sum, na.rm = TRUE),
             end_m_trips = end_trips / 1e6L,
             start_m_trips = start_trips / 1e6L)
  ][, unbalance_situation := fcase(
    ending_starting_rate < quantile(ending_starting_rate, 0.15),
    "More starts than ends",
    ending_starting_rate > quantile(ending_starting_rate, 0.85),
    "More ends than starts",
    default = "Balanced"
  )
  ][order(-(start_trips + end_trips)), 
    .SD[1:6],
    by = "borough"] 

# Creating the scatted plot
ggplot(ZonesCountToPlot,
       aes(start_m_trips, end_m_trips))+
  geom_blank(aes(pmax(start_m_trips, end_m_trips),
                 pmax(start_m_trips, end_m_trips)))+
  geom_abline(linewidth = 0.8,
              alpha = 0.5)+
  geom_point(aes(color = borough),
             size = 3.5,
             alpha = 0.75)+
  geom_text(data = ZonesCountToPlot[unbalance_situation ==
                                      "More starts than ends"],
            aes(label = zone),
            size = 3.5,
            hjust = -0.12,
            check_overlap = TRUE)+
  geom_text(data = ZonesCountToPlot[unbalance_situation ==
                                      "More ends than starts"],
            aes(label = zone),
            size = 3.5,
            hjust = 1.12,
            check_overlap = TRUE)+
  scale_x_continuous(labels = comma_format(accuracy = 0.1, suffix = " M"))+
  scale_y_continuous(labels = comma_format(accuracy = 0.1, suffix = " M"))+
  coord_equal() +
  labs(title = "Top 6 Most Important Zones by Borough",
       color = "Borough",
       x = "Number of Trips Starting",
       y = "Number of Trips Ending")+
  theme_light()+
  theme(legend.position = "top",
        text = element_text(color = "black"),
        plot.title = element_text(face = "bold"))
```

Based on the results, we can highlight the next points:

1.  The airports present in Queens, *LaGuardia Airport* and *JFK Airport*, have many more trips going to the airport than going out of airport. This might happen due that there are more transportation options like other taxis, shuttles, and public transportation.

2.  The remaining zones, *Jackson Heights*, *East Village* and *TriBeCa/Civic Center*, are residential zones with a variety of public transportation options.

---
title: "Data Understanding"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Understanding}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
params:
  UpdateSample: FALSE
  UpdateGeoPoints: FALSE
  MaxTripHours: 3
  MaxDriverPay: 300
  MaxLimitTripMiles: 300
---

```{r, include = FALSE}
TrainingPath <- here::here("raw-data/TrainingTrips.fst")
ExistTrainingData <- file.exists(TrainingPath)

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library('project.nyc.taxi')
```

```{r include=FALSE, eval=!params$UpdateSample}
TrainingTrips <- fst::read_fst(TrainingPath, as.data.table = TRUE)
```

```{r eval=!exists("ZoneCodesArcgis")}
load(here::here("data/ZoneCodesArcgis.rda"))

ZoneCodesArcgisClean <- clean_zone_manually(ZoneCodesArcgis)
```

After completing the [business understanding](https://angelfelizr.github.io/project.nyc.taxi/articles/02-business-understanding.html) phase we are ready to perform the **data understanding** phase by performing an EDA with the following steps:

1.  Exploring the individual distribution of variables.
2.  Exploring correlations between predictors and target variable.
3.  Exploring correlations between predictors.

In order to:

-   Ensure data quality
-   Identify key predictors
-   Detect multicollinearity
-   Guide model choice and feature engineering

This can include 
- investigations of correlation structures in the variables,
- patterns of missing data,
- anomalous motifs in the data that might challenge the initial expectations of the modeler.

## Setting the environment up

To setting the `R` environment up we just need to apply the following 5 steps:

1.  Loading the packages to use.

```{r warning = FALSE, message = FALSE}
## To manage relative paths
library(here)

## To transform data larger than RAM
library(dplyr)
library(arrow)

## To transform data that fits in RAM
library(data.table)
library(lubridate)

## To create plots
library(ggplot2)
library(scales)


options(datatable.print.nrows = 15,
        digits = 4)
```

2.  Creating an Arrow connection object to perform some **manipulations in disk** before taking the data into the RAM memory.

```{r}
data_path <- here("raw-data/trip-data/year=2022")

NycTrips2022 <- open_dataset(data_path)

dim(NycTrips2022) |> comma()

NycTrips2022
```

3.  Importing the zone code description with `lat` and `long`.

```{r, eval=params$UpdateGeoPoints}
ZoneCodesArcgis <- 
  fread(here("raw-data/taxi_zone_lookup.csv"),
        colClasses = c("integer",
                       "character",
                       "character",
                       "character")
  )[, Address := paste(Zone,
                       Borough,
                       "New York",
                       "United States",
                       sep = ", ")
  ][, tidygeocoder::geocode(.SD,
                            address = Address,
                            method = 'arcgis')]

setDT(ZoneCodesArcgis)

glimpse(ZoneCodesArcgis)
```

```{r, echo=FALSE}

if(params$UpdateGeoPoints){
  
  usethis::use_data(ZoneCodesArcgis,
                    overwrite = TRUE,
                    compress = "xz",
                    version = 3)
  
}else{
  
  glimpse(ZoneCodesArcgis)

}

```

4. Validating the added latitude and longitude, by plotting all the points in an interactive map from we can highlight the following issues:

  - The location suggested for the Newark Airport is not correct.
  - More than one zone is represented by one location.

```{r}
BoroughColors <- c(
  'Manhattan' = '#e41a1c',
  'Queens' = '#377eb8',
  'Brooklyn'= '#4daf4a',
  'Bronx' = '#984ea3',
  'Staten Island' = '#ff7f00',
  'EWR' = '#a65628'
)

plot_map(
  ZoneCodesArcgis,
  lng_var = "long",
  lat_var = "lat",
  color_var = "Borough",
  color_palette = BoroughColors,
  radius = 3,
  label_var = "Zone"
)
```

Now we can see the new map after solving the prior problems.

```{r}
ZoneCodesArcgisClean <- clean_zone_manually(ZoneCodesArcgis)

plot_map(
  ZoneCodesArcgisClean,
  lng_var = "long",
  lat_var = "lat",
  color_var = "Borough",
  color_palette = BoroughColors,
  radius = 3,
  label_var = "Zone"
)
```

5.  Counting the number of trips for each code, collecting and translating the zone codes.

```{r}
TripsZoneDistribution <-
  NycTrips2022 |>
  count_pct(PULocationID, DOLocationID) |>
  add_zone_description(zone_dt = ZoneCodesArcgisClean,
                       start_id_col = "PULocationID",
                       end_id_col = "DOLocationID",
                       zone_id_col = "LocationID")

glimpse(TripsZoneDistribution)
```

## Individual distributions

### company

The majority number of trips are done by *Uber* (HV003) and the rest for *Lyft*.

```{r}
NycTrips2022 |> 
  count_pct(company = case_when(hvfhs_license_num == "HV0002" ~ "Juno",
                                hvfhs_license_num == "HV0003" ~ "Uber",
                                hvfhs_license_num == "HV0004" ~ "Via",
                                hvfhs_license_num == "HV0005" ~ "Lyft"))
```

### dispatching_base_num

For most of the trips the dispatching number are **B03404** and **B03406**. The remaining codes represent a small proportion of observations.

```{r}
NycTrips2022 |>
  count_pct(dispatching_base_num)
```

### originating_base_num

For most of the trips the originating number is **B03404** and the second is missing. The remaining codes represent a small proportion of observations.

```{r}
NycTrips2022 |>
  count_pct(originating_base_num)
```

### access_a_ride_flag

Must of the trips doesn't provide any information related to whether they were administered on behalf of the Metropolitan Transportation Authority (MTA) and the remaining confirm that there aren't.

```{r}
NycTrips2022 |>
  count_pct(access_a_ride_flag)
```

### shared_request_flag

Most of passengers **don't request** to a shared/pooled ride.

```{r}
NycTrips2022 |>
  count_pct(shared_request_flag)
```

### shared_match_flag

Most of passengers don't request to a shared/pooled ride, but even **fewer achieved to share the ride**.

```{r}
NycTrips2022 |> 
  count_pct(shared_match_flag)
```

### wav_request_flag

It's really **unusual to request a wheelchair-accessible** vehicle.

```{r}
NycTrips2022 |> 
  count_pct(wav_request_flag)
```

### wav_match_flag

It's really unusual to request a wheelchair-accessible vehicle, but we have more tips take place in wheelchair-accessible vehicles, so **it seems that the service has the capacity to meet the demand**.

```{r}
NycTrips2022 |> 
  count_pct(wav_match_flag)
```

### start_service_zone

**96% of trips** start in *Boro Zone* and *Yellow Zone*.

```{r}
TripsZoneDistribution |>
  count_pct(start_service_zone, wt =  n)
```

### end_service_zone

**91% of trips** start in *Boro Zone* and *Yellow Zone*.

```{r}
TripsZoneDistribution |>
  count_pct(end_service_zone, wt =  n)
```

### start_borough

**87% of trips** start in *Manhattan*, *Brooklyn* or *Queens*.

```{r}
TripsZoneDistribution |>
  count_pct(start_borough, wt =  n)
```

### start_zone {.tabset}

To gain a deeper understanding of the starting trips at the zone level, we'll create a **wrapper function** to display the top 5 most significant zones and generate an interactive map.

The map will illustrate the location of each zone in relation to the remaining locations within each borough.

#### Manhattan

Here we can see how the _East Village_ zone is surrounded by _(Meatpacking) West Village_ and _Clinton (East|West)_, so most of the activity happens in those places. The remaining top zones ( _Upper East Side (North|South)_ and _Central Harlem (North)_ ) are surrounding the **Central Park**.

All these places are very popular for local and international tourist.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Manhattan",
                    borough_color = BoroughColors[["Manhattan"]],
                    top_color = "purple",
                    col_prefix  = "start_")
```

#### Brooklyn

In this Borough, the most important starting zones are concentrated in the north part. _Bushwick (North|South)_, _Crown Heights (North|South)_, _Williamsburg (North Side)_ and _Park Slope_ form an **"U"** shape, the farthest zone is the _East New York_.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Brooklyn",
                    borough_color = BoroughColors[["Brooklyn"]],
                    top_color = "purple",
                    col_prefix  = "start_")
```

#### Queens

Most of the most important starting zones are concentrated in north part of this Borough and the only exception is the _JFK Airport_ in the south.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Queens",
                    borough_color = BoroughColors[["Queens"]],
                    top_color = "purple",
                    col_prefix  = "start_")
```

### end_borough

**82% of trips** end in *Manhattan*, *Brooklyn* or *Queens*.

```{r}
TripsZoneDistribution |>
  count_pct(end_borough, wt =  n)
```

### end_zone {.tabset}

#### Manhattan

If we focus our attention to the places where the trips end we see the new zone _East Harlem (North|South)_ which get the top 5 moving the _East Village_ to the sixth position.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Manhattan",
                    borough_color = BoroughColors[["Manhattan"]],
                    top_length = 6,
                    top_color = "purple",
                    col_prefix  = "end_")
```

#### Brooklyn

We also can see the same pattern from the ending zone perspective.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Brooklyn",
                    borough_color = BoroughColors[["Brooklyn"]],
                    top_color = "purple",
                    col_prefix  = "end_")
```

#### Queens

For the ending perspective, now the _JFK Airport_ present more trips than the _LaGuardia Airport_.

```{r}
highlight_top_zones(TripsZoneDistribution,
                    borough = "Queens",
                    borough_color = BoroughColors[["Queens"]],
                    top_color = "purple",
                    col_prefix  = "end_")
```

### request_date

To describe this variable, we decomposed it in different parts and count the number trips by each element and store the summary as a `data.table` to explore each part using visualizations.

```{r}
RequestTimeSummary <-
  NycTrips2022 |>
  mutate(request_date = as_date(request_datetime)) |>
  count(request_month = floor_date(request_date, unit = "month"),
        request_week = floor_date(request_date, unit = "week"),
        request_day = day(request_date),
        request_weekday = wday(request_date, week_start = 1),
        request_hour = hour(request_datetime)) |>
  collect() |>
  as.data.table()
```

#### request_datetime by month

In the next chart, we can see that the number trips keeps almost constant must of the year, but we have some fewer trips during the first 2 months and some extra trips in October and December.

```{r}
RequestTimeSummary[year(request_month) == 2022, 
                   .(n = sum(n)),
                   by = "request_month"] |>
  ggplot(aes(request_month, n))+
  geom_line(color = "grey60",
            linewidth = 0.9)+
  geom_point(color = "dodgerblue4",
             size = 3)+
  scale_x_date(date_labels = "%y-%m",
               date_breaks = "2 months")+
  scale_y_continuous(labels = comma_format(),
                     breaks = breaks_width(2e6))+
  labs(title = "Distribution of Trips by Month",
       x = "Trips Request Month",
       y = "Number of Trips")+
  expand_limits(y = 0)+
  theme_light()+
  theme(plot.title = element_text(face = "bold", hjust = 0.5),
        panel.grid.minor = element_blank(),
        legend.position = "top")
```

#### request_datetime by week

By breaking the months into weeks we can confirm we have fewer trips in the first 2 months, in relation to October we don't see a big change in he number of trips we see is that it has more weeks than November, but December keeps having more trips than normal in the first 2 weeks.

```{r}
RequestTimeSummary[year(request_month) == 2022, 
                   .(n = sum(n)),
                   by = "request_week"] |>
  ggplot(aes(request_week, n))+
  geom_line(color = "grey60",
            linewidth = 0.9)+
  geom_point(color = "dodgerblue4",
             size = 3)+
  scale_x_date(date_labels = "%y-%m",
               date_breaks = "month")+
  scale_y_continuous(labels = comma_format(),
                     breaks = breaks_width(5e5))+
  labs(title = "Distribution of Trips by Week",
       x = "Trips Request Week",
       y = "Number of Trips")+
  expand_limits(y = 0)+
  theme_light()+
  theme(plot.title = element_text(face = "bold", hjust = 0.5),
        panel.grid.minor = element_blank(),
        legend.position = "top")
```

#### request_datetime by month day

If we explore the number of trips by month day we can not see any consistent pattern after plotting a line with total of trips for each month.

```{r}
RequestTimeSummary[year(request_month) == 2022, 
                   .(n = sum(n)),
                   by = .(request_month = format(request_month, "%B"),
                          request_day)] |>
  ggplot(aes(request_day, n))+
  geom_line(aes(group = request_month),
            color = "gray60",
            linewidth = 0.1)+
  geom_smooth(method = 'loess',
              formula = 'y ~ x',
              se = FALSE,
              linewidth = 1.2)+
  scale_x_continuous(breaks = breaks_width(5))+
  scale_y_continuous(labels = comma_format())+
  expand_limits(y = 0)+
  labs(title = "Number of Trips by Month Day",
       y = "Number of Trips",
       x = "Day of Month")+
  theme_light()+
  theme(panel.grid = element_blank(),
        plot.title = element_text(face = "bold"))
```

#### request_datetime by week day

By if we change the month day in the prior chart with week day we can find that the number of trips trends to be higher Fridays and Saturdays.

```{r,warning=FALSE}
RequestTimeSummary[year(request_month) == 2022, 
                   .(n = sum(n)),
                   by = .(request_month = format(request_month, "%B"),
                          request_weekday)] |>
  ggplot(aes(request_weekday, n))+
  geom_line(aes(group = request_month),
            color = "gray60")+
  geom_smooth(method = 'loess',
              formula = 'y ~ x',
              se = FALSE,
              linewidth = 1.2)+
  scale_x_continuous(breaks = breaks_width(1),
                     labels = factor_weekday)+
  scale_y_continuous(labels = comma_format())+
  expand_limits(y = 0)+
  labs(title = "Number of Trips by Week Day",
       y = "Number of Trips",
       x = "Day of Week")+
  theme_light()+
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))
```

#### request_datetime by week day and hour

To understand better what is happening Fridays and Saturdays let's break each week day by hour. In the next chart, we can see how the higher number of trips start at 17:00 and end at 1:00 of next day for Fridays and Saturdays.

```{r}
RequestTimeSummary[year(request_month) == 2022, 
                   .(n = sum(n)),
                   by = .(request_hour = 
                            factor(request_hour) |> fct_rev(), 
                          request_weekday = factor_weekday(request_weekday))
  ][, n_million := n/1e6 ] |>
  ggplot(aes(request_weekday, request_hour))+
  geom_tile(aes(fill = n),
            color = "black",
            linewidth = 0.005)+
  geom_text(aes(label = comma(n_million, accuracy = 0.1, suffix = " M")))+
  scale_fill_gradient(low = "white", 
                      high = "dodgerblue4",
                      labels= comma_format())+
  scale_x_discrete(position = "top") +
  labs(title = "Number of Trips by Hour and Week Day",
       fill = "Number of Trips",
       x = "Request Week Day",
       y = "Request Hour") +
  theme_classic() +
  theme(plot.title = element_text(face = "bold"),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        axis.text = element_text(color = "black"))
```

### dropoff_datetime

We know that a taxi trip takes more less than a day doesn't make more sense to explore this variable. Instead, it is better to consecrate the efforts to understand the distribution of the difference between `request_datetime` and `dropoff_datetime` as the new variable **trip_hours**.

### trip_hours

Most of the trips take less than an hour, which make sense, but having trips with **negative 5 hours** or longer than **41 hours** looks a quality problem.

```{r}
NycTrips2022 |>
  mutate(trip_hours = arrow_minutes_between(request_datetime, dropoff_datetime) / 60) |>
  compute_num_summary(trip_hours)
```

Trips that takes longer than **`r params$MaxTripHours` hours** doesn't look right, so we are going to investigate more before adding that data to the model.

```{r}
NycTrips2022 |>
  mutate(trip_hours = arrow_minutes_between(request_datetime, dropoff_datetime) / 60) |>
  count_pct(trip_hours_status = case_when(trip_hours < 0 ~ "Negative",
                                          trip_hours <= 1   ~ "[0:00, 1:00]",
                                          trip_hours <= 2 ~ "(1:00, 2:00]",
                                          trip_hours <= 3 ~ "(2:00, 3:00]",
                                          trip_hours <= 4 ~ "(3:00, 4:00]",
                                          trip_hours <= 5 ~ "(4:00, 5:00]",
                                          trip_hours <= 6 ~ "(5:00, 6:00]",
                                          trip_hours <= 7 ~ "(5:00, 7:00]",
                                          trip_hours <= 8 ~ "(7:00, 8:00]",
                                          trip_hours <= 9 ~ "(8:00, 9:00]",
                                          trip_hours <= 10 ~ "(9:00, 10:00]",
                                          trip_hours > 10 ~ ">10 hours"),
            digits = 6L)
```

### driver_pay

Just by checking the summary statistics we can see to important problems:

-   A base fare can not be negative
-   A base of almost $4.6k is too much

```{r}
NycTrips2022 |>
  compute_num_summary(driver_pay)
```

By checking the closer to the distribution we can confirm that 95% of trips present values from 5 to 50 dollars, but any fare higher than **`r scales::comma(params$MaxDriverPay)` dollars** seems weird won't be used to train the model.

```{r}
NycTrips2022 |>
  count_pct(base_passenger_fare_interval = 
              case_when(driver_pay >= 4000 ~ ">=4000",
                        driver_pay >= 3000 ~ "[3000, 4000)",
                        driver_pay >= 2000 ~ "[2000, 3000)",
                        driver_pay >= 1000 ~ "[1000, 2000)",
                        driver_pay >= 750 ~ "[750, 1000)",
                        driver_pay >= 500 ~ "[500, 750)",
                        driver_pay >= 400 ~ "[400, 500)",
                        driver_pay >= 300 ~ "[300, 400)",
                        driver_pay >= 200 ~ "[200, 300)",
                        driver_pay >= 100 ~ "[100, 200)",
                        driver_pay >= 50 ~ "[50, 10)",
                        driver_pay >= 5 ~ "[5, 50)",
                        driver_pay >= 0 ~ "[0, 5)",
                        TRUE ~ "<0"),
            digits = 8L)
```


### trip_miles

Below we can see that the min distance was **0 miles** which can be possible if the trip duration was also short, but we need to check those cases and the higher distance was **634 miles**, which can be a valid outlier taking in consideration that the higher whisker is 13.2 miles.

```{r}
NycTrips2022 |>
  compute_num_summary(trip_miles)
```

We can also check that only 24.4K trips are over 100 miles.

```{r}
NycTrips2022 |>
  count_pct(trip_miles_status = case_when(trip_miles < 100 ~ "Normal trips",
                                          trip_miles >= 100 ~ "Long Trips"),
            digits = 4L)
```

But even just taking the long trips we can see that 98% of those trips were shorter than **`r scales::comma(params$MaxLimitTripMiles)` miles**, those trips doesn't look like good examples to train the model.

```{r}
NycTrips2022 |>
  filter(trip_miles >= 100) |>
  count_pct(miles_interval = case_when(trip_miles >= 600 ~ ">=600",
                                       trip_miles >= 500 ~ "[500, 600)",
                                       trip_miles >= 400 ~ "[400, 500)",
                                       trip_miles >= 300 ~ "[300, 400)",
                                       trip_miles >= 200 ~ "[200, 300)",
                                       trip_miles >= 150 ~ "[150, 200)",
                                       trip_miles >= 130 ~ "[130, 150)",
                                       trip_miles >= 115 ~ "[115, 130)",
                                       trip_miles >= 100 ~ "[100,115)"),
            digits = 4L)
```

### tips

The column looks really good, it makes sense that for more than 75% of trips the tip is 0 as it is not mandatory, but having a trip with a \$294 tip is not hard to believe.

```{r}
NycTrips2022 |>
  compute_num_summary(tips)
```

After braking the `tips` we can see that `80%` of trips don't present any tip and must of the tips are lower than **50 dollars**, so gets really hard to achieved more than that based on tips.

```{r}
NycTrips2022 |>
  count_pct(tips_interval = case_when(tips >= 250 ~ ">=250",
                                      tips >= 200 ~ "[200, 250)",
                                      tips >= 150 ~ "[150, 200)",
                                      tips >= 100 ~ "[100, 150)",
                                      tips >= 50 ~ "[50, 100)",
                                      tips > 0 ~ "(0, 50)",
                                      TRUE ~ "0"),
            digits = 8L)
```

## Sampling data

Despite the amazing the capacities of arrow to summarize larger than RAM data, to continue the analysis in the project we will need to take a sample in order to load the data in the memory.

To sample the data in a reproducible way, we are going to apply the next steps using the `sample_parquet()` function on each parquet file previously downloaded:

1. Importing the whole parquet file into memory.
2. Selecting the number 1 as seed.
3. Filtering out any zone out of **Manhattan**, **Brooklyn** and **Queens** to limit the scope of the project where 80% of the activity takes place.
4. Filtering at random the a number that rows needed to represent 1% of the data.

```{r, eval=params$UpdateSample}
ValidZoneCombinations <- 
  ZoneCodesArcgisClean[Borough %chin% c("Manhattan", "Brooklyn", "Queens"),
                       unique(LocationID)] |>
  (\(x) CJ(PULocationID = x,
           DOLocationID = x) )()

TrainingTrips <-
  dir(data_path,
      full.names = TRUE,
      recursive = TRUE) |>
  lapply(sample_parquet,
         valid_combinations = ValidZoneCombinations,
         prob = 0.01) |>
  rbindlist()

nrow(TrainingTrips) |> comma()
```

```{r, include=FALSE}
if(params$UpdateSample){
  fst::write_fst(TrainingTrips,
                 "raw-data/TrainingTrips.fst",
                 100)
}else{
  nrow(TrainingTrips) |> comma()
}
```

## Solving quality problems

Thanks to the initial exploration, we know that we need to apply the following transformation before start exploring the correlations between predictors and target variable:

1. Creating the `company` column based on the `hvfhs_license_num` to ensure data understanding.

2. Keeping the **B03404** and **B03406** as valid `dispatching_base_num` and transforming the remaining categories using the label **Other**.

3. Keeping the **B03404** and **Missing** as valid `originating_base_num` and transforming the remaining categories using the label **Other**.

4. Making the empty category as **Missing** to make explicit the value provided for the `access_a_ride_flag` column.

5. Exchanging the `request_datetime` and `dropoff_datetime` when the resulted `trip_hours` ends in a negative number.

6. Replacing any negative `driver_pay` with `NA`.

And we need to repeat that process for the training and test data apply all those changes using the `set_initial_cleaning()` to edit the data without making any copy in RAM.

```{r}
TrainingTripsCleaned <- 
  TrainingTrips[, .SD, .SDcols = list_cols_to_use()] |>
  add_zone_description(zone_dt = ZoneCodesArcgisClean,
                       start_id_col = "PULocationID",
                       end_id_col = "DOLocationID",
                       zone_id_col = "LocationID")

set_initial_cleaning(TrainingTripsCleaned)

rm(TrainingTrips)

glimpse(TrainingTripsCleaned)
```

## Do extreme values make sense?

Before removing any extreme value is important to confirm if they make sense.

We expect that high values of `driver_pay` (over `r scales::comma(params$MaxDriverPay)` dollars) are also related with high values of `trip_hours` (over `r params$MaxTripHours` hours) or `trip_miles` (over `r scales::comma(params$MaxLimitTripMiles)` miles), so save a data.frame with those values.

```{r}
ExtremeValues <-
  TrainingTripsCleaned[driver_pay >= params$MaxDriverPay |
                         trip_hours >= params$MaxTripHours |
                         trip_miles >= params$MaxLimitTripMiles,
                       .(driver_pay, trip_hours, trip_miles)]
```

As we can see below the in the Adjusted R-squared,a this simple model can explain 67% of the variance.

```{r}
DriverPayLm <- lm(driver_pay ~ ., data = ExtremeValues)

summary(DriverPayLm)
```

To see in which cases the model doesn't fit with data we can will plot the model predictions against the original values and as we can see below the main problem to gain a better fit the observations which present more than \$750 of `driver_pay` where the model was expecting almost \$500.

```{r}
ggplot(broom::augment(DriverPayLm),
       aes(.fitted , driver_pay)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, lty = 5) +
  scale_y_continuous(breaks = breaks_width(100)) +
  coord_fixed() +
  theme_minimal() +
  theme(panel.grid.minor = element_blank())
```

But even thought this model cannot explain that high value, I could be right as another variable might explain that increase later, it was really a long trip of 6 hours and more than 200 miles, so **we will keep all the extreme values as part of the model**.

```{r}
ExtremeValues[driver_pay > 900]
```

## Can we delete rows with missing values?


```{r}
model <- lm(driver_pay ~ ., data = TrainingTripsCleaned)

vif_results <- car::vif(c)

naniar::mcar_test(as.data.frame(TrainingTripsCleaned))

mice::md.pattern(TrainingTripsCleaned, rotate.names = TRUE) |> invisible()
```


## Correlations between predictors and target variable

Before exploring correlations we need to transforming the sampled data by:

-   Solving quality problems.
-   Transforming categorical variables into boolean ones.
-   Transforming zone code to factors.
-   Adding new features.

## start_service vs end_service zones

```{r}
#| echo: false
#| include: false

ZoneUniqueComb <-
  TripsZoneDistribution[ 
    !start_borough %chin% c("Staten Island", "Unknown", "EWR") & 
      !end_borough %chin% c("Staten Island", "Unknown", "EWR"),
    unique(.SD),
    .SDcols = c("start_zone", "end_zone")
  ][, .N] |>
  comma()

EndingUniqueZones <-
  TripsZoneDistribution[ 
    !start_borough %chin% c("Staten Island", "Unknown", "EWR") & 
      !end_borough %chin% c("Staten Island", "Unknown", "EWR"),
    uniqueN(end_zone),] |>
  comma()
```

-   `start_zone` and `end_zone`: As our data has `r ZoneUniqueComb` rows of relations between both columns, we opted to transform data in a way what each unique zone represent a row reducing the points to be plotted to only `r EndingUniqueZones` by following the next steps:

    1.  Summarizing the total number of trips for each **starting point** independently to its destination
    2.  Summarizing the total number of trips for each **ending point** independently to its origin.
    3.  Joining both tables into one.

```{r}
# 1. Summarizing Staring Zones
StartingZonesCount <-
  TripsZoneDistribution[
    !start_borough %chin% c("Staten Island", "Unknown", "EWR") &
      !end_borough %chin% c("Staten Island", "Unknown", "EWR"),
    .(start_trips = sum(n)),
    by =  .(borough = start_borough, 
            zone = start_zone)
  ]

# 2. Summarizing Ending Zones
EndingZonesCount <-
  TripsZoneDistribution[
    !start_borough %chin% c("Staten Island", "Unknown", "EWR") &
      !end_borough %chin% c("Staten Island", "Unknown", "EWR"), 
    .(end_trips = sum(n)),
    by =  .(borough = end_borough, 
            zone = end_zone)
  ]

# 3. Inner Joining Starting and Ending Zones Counts
JoinedZonesCount <-
  StartingZonesCount[
    EndingZonesCount,
    on = c("borough", "zone"),
    nomatch = 0
  ]
```

Once we have a much simpler data to work with, we is easy to confirm with the next linear regression that `start_trips` and `end_trips` has almost the same values the model has an slope of one. That means that in must of cases **if someone takes a taxi to go to any place it's really likely to take another taxi back**.

```{r}
lm(end_trips ~ start_trips, 
   data = JoinedZonesCount) |>
  summary()
```

Let's now explore the zones where there is **no balance** between the `start_trips` and the `end_trips` in the most visited zoned of each Borough. To do so, we defined the rate `end_trips`/`start_trips` and highlight zones with lower rate than the 15% percentile or higher rate than 85% percentile.

```{r}
# Creating dataset to plot
ZonesCountToPlot <-
  copy(JoinedZonesCount)[
    j = `:=`(ending_starting_rate = end_trips/start_trips,
             borough = fct_reorder(borough, -end_trips, .fun = sum, na.rm = TRUE),
             end_m_trips = end_trips / 1e6L,
             start_m_trips = start_trips / 1e6L)
  ][, unbalance_situation := fcase(
    ending_starting_rate < quantile(ending_starting_rate, 0.15),
    "More starts than ends",
    ending_starting_rate > quantile(ending_starting_rate, 0.85),
    "More ends than starts",
    default = "Balanced"
  )
  ][order(-(start_trips + end_trips)), 
    .SD[1:6],
    by = "borough"] 

# Creating the scatted plot
ggplot(ZonesCountToPlot,
       aes(start_m_trips, end_m_trips))+
  geom_blank(aes(pmax(start_m_trips, end_m_trips),
                 pmax(start_m_trips, end_m_trips)))+
  geom_abline(linewidth = 0.8,
              alpha = 0.5)+
  geom_point(aes(color = borough),
             size = 3.5,
             alpha = 0.75)+
  geom_text(data = ZonesCountToPlot[unbalance_situation ==
                                      "More starts than ends"],
            aes(label = zone),
            size = 3.5,
            hjust = -0.12,
            check_overlap = TRUE)+
  geom_text(data = ZonesCountToPlot[unbalance_situation ==
                                      "More ends than starts"],
            aes(label = zone),
            size = 3.5,
            hjust = 1.12,
            check_overlap = TRUE)+
  scale_x_continuous(labels = comma_format(accuracy = 0.1, suffix = " M"))+
  scale_y_continuous(labels = comma_format(accuracy = 0.1, suffix = " M"))+
  coord_equal() +
  labs(title = "Top 6 Most Important Zones by Borough",
       color = "Borough",
       x = "Number of Trips Starting",
       y = "Number of Trips Ending")+
  theme_light()+
  theme(legend.position = "top",
        text = element_text(color = "black"),
        plot.title = element_text(face = "bold"))
```


investigations of correlation structures in the variables, patterns of missing data, and/or anomalous 

Based on the results, we can highlight the next points:

1.  The airports present in Queens, *LaGuardia Airport* and *JFK Airport*, have many more trips going to the airport than going out of airport. This might happen due that there are more transportation options like other taxis, shuttles, and public transportation.

2.  The remaining zones, *Jackson Heights*, *East Village* and *TriBeCa/Civic Center*, are residential zones with a variety of public transportation options.

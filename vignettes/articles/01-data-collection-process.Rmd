---
title: "Data Collection Process"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Collection Process}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

For most projects the data collection process can be done manually and later attache the file in a folder but that isn't a option when we are working with **big data**.

To solve this problem, we have created the next script to automate the data collection process so the project could be reproduced easily just by running the code below.


## Web Scraping

To always have a updated list of 2022 and 2023 links of **High Volume For-Hire Vehicles** documents let's scrape the [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) by using the `rvest` library.

### Downloading source page

```{r SourcePage}
SourcePage <-
  rvest::read_html("https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page")
```


### Getting links for taxi trips

```{r getting-trip-links}
TripLinks <-
  SourcePage |>
  rvest::html_elements(xpath = '//div[@class="faq-answers"]//li/a') |>
  rvest::html_attr("href") |>
  grep(pattern = "fhvhv_[a-z]+_202[23]-\\d{2}\\.parquet", value = TRUE) |>
  trimws() |>
  sort()

FileNames <- basename(TripLinks)

FileNames
```

### Getting link for trip zones.

```{r getting-taxi-zone-link}
TaxiZoneLink <-
  SourcePage |>
  rvest::html_elements(xpath = '//ul/li/a[text()="Taxi Zone Lookup Table"]')  |>
  rvest::html_attr("href") |>
  trimws()

TaxiZoneLink
```

## Saving trips data

To take advantage of the best capacities of `Duckdb` we need save a each parquet file in folder with useful information to filter later, that why we will have one folder level related to years the next sub-folders related to a month with each parquet with the name `part-0.parquet` by following process.

### Defining folder paths

```{r data-paths}
SourcePath <- here::here()

RawDataPath <- file.path(SourcePath, "raw-data")

ParquetFolderPath <- file.path(RawDataPath, "trip-data")

YearFoldersPath <- 
  gsub(x = FileNames,
       pattern = "^fhvhv_tripdata_|-\\d{2}\\.parquet$",
       replacement = "") |>
  paste0("year=", a = _) |>
  unique() |>
  file.path(ParquetFolderPath, a = _)

```

### Creating folders to use

```{r creating-folders}
for(paths in c(RawDataPath, ParquetFolderPath, YearFoldersPath)) {
  
  if(!file.exists(paths)){
    dir.create(paths)
  }
  
}
```

### Defining the sub-folders to split the files based on year.

```{r count-parquet-files, include=FALSE}
HasAllParquet <- length(list.files(ParquetFolderPath, recursive = TRUE)) == 24L
```

```{r parsermd-chunk-5, eval=!HasAllParquet}
for(year_i in YearFoldersPath) dir.create(year_i, showWarnings = FALSE)
```

### Creating a folder for each month

```{r parsermd-chunk-6, eval=!HasAllParquet}
MonthFolders <- gsub(
  x = FileNames,
  pattern = "^fhvhv_tripdata_\\d{4}-|\\.parquet$",
  replacement = ""
) |>
  paste0("month=", a = _)

MonthFoldersPath <- file.path(ParquetFolderPath, YearFolders, MonthFolders)

for(month_i in MonthFoldersPath) dir.create(month_i, showWarnings = FALSE)
```

### Downloading each file on each folder

```{r eval=!HasAllParquet}
# Parquet files might time a longer time to be downloaded
options(timeout = 1800)


# Parquet trip data
for(link_i in seq_along(TripLinks)){
  
  download.file(TripLinks[link_i],
                destfile = file.path(MonthFoldersPath[link_i],"part-0.parquet"),
                mode = "wb")
  
}


# Taxi Zone CSV
download.file(TaxiZoneLink,
              destfile = file.path(RawDataPath,"taxi_zone_lookup.csv"),
              mode = "wb")
```

## Saving results in a database

Despite that parquet files are great for sharing big data files keeping save the format for each format, it presents problems if we need to apply **sampling operations**.

To work with data large than RAM, we will need to create a simple data base with `duckdb` by following the next simple steps.

1. Listing all files to read.

```{r}
ParquetSource <-
  list.files(ParquetFolderPath,
             recursive = TRUE,
             full.names = TRUE) |>
  paste0("'", a = _ ,"'") |> 
  paste0(collapse = ", ") |>
  paste0("read_parquet([", a = _ ,"])")

ParquetSource
```

2. Creating a connection.

```r
con <- DBI::dbConnect(duckdb::duckdb(), 
                      dbdir = here::here("my-db.duckdb"))
```

3. Saving all files in the database after adding an id to differentiate between trips and adding the performance per our each trips.

```r
NycTripsCreateTable <- glue::glue_safe("
CREATE TABLE NycTrips AS
    SELECT 
        ROW_NUMBER() OVER () AS trip_id,
        *,
        (driver_pay + tips) / (trip_time / 3600) AS performance_per_hour
    FROM {ParquetSource}
")

DBI::dbExecute(con, NycTripsCreateTable)
```

4. Disconnecting the data base.

```r
DBI::dbDisconnect(con, shutdown = TRUE)

rm(con)
```


5. After saving the data in the data base we can confirm its final size.

```{r}
here::here("my-db.duckdb") |>
  file.size() |>
  structure(class = "object_size") |>
  format(units = "auto")
```

